{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Import modules and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T16:25:11.348168Z",
     "start_time": "2023-07-14T16:25:06.838456Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from utils import *\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def getPermutedTensor(factors, lambdas, tensorX, NDIRS):\n",
    "    \n",
    "    \"\"\"Find the optimal circular-shifts used by the permuted decomposition to produce\n",
    "    the tensor components, and apply it to the original tensor.\"\"\"\n",
    "    \n",
    "    # Compute reconstructed tensor by scaling the first mode by the lambdas and \n",
    "    # multiplying by the kathri rao product of the other modes\n",
    "    fittensor = np.reshape((lambdas * factors[0]) @ khatri_rao(factors[1:]).T, tensorX.shape)\n",
    "\n",
    "    if NDIRS == 1: #no shifting possible, so simply return original tensor\n",
    "        return tensorX, fittensor\n",
    "\n",
    "    N = tensorX.shape[0]\n",
    "    NSTIMS = tensorX.shape[1]\n",
    "    RLEN = tensorX.shape[2]\n",
    "\n",
    "    shape4d = (N,NSTIMS,NDIRS,RLEN//NDIRS)\n",
    "    shapeDot = (N,RLEN)\n",
    "    tensor4d = np.reshape(tensorX,shape4d,order='F')\n",
    "\n",
    "    objs = np.empty((NSTIMS,N,NDIRS))\n",
    "    obj_shifts = np.empty((NSTIMS,N))\n",
    "    #find best shift (argmin) per stim for all cells at once\n",
    "    for si in range(NSTIMS):\n",
    "        for shifti in range(NDIRS):\n",
    "            # cf. matlab code in `permuted-decomposition/matlab/my_tt_cp_fg.m`\n",
    "            objs[si,:,shifti] = -np.sum(fittensor[:,si,:] * np.reshape(np.roll(tensor4d[:,si],shifti,1),shapeDot,order='F'), 1)\n",
    "        obj_shifts[si] = np.argmin(objs[si],axis=1)\n",
    "\n",
    "    #apply shifts\n",
    "    shifted_tensor = np.zeros_like(tensorX)\n",
    "    for shifti in range(NDIRS):\n",
    "        rolledX = np.reshape(np.roll(tensor4d,shifti,2), tensorX.shape, order='F')\n",
    "        for si in range(NSTIMS):\n",
    "            shifted_tensor[(obj_shifts[si] == shifti),si,:] = rolledX[(obj_shifts[si] == shifti),si,:]\n",
    "\n",
    "    #check that we get the same fit -- OK\n",
    "    # normsqX = np.square(norm(tensorX.ravel()))\n",
    "    # print((np.square(norm(shifted_tensor.ravel() - fittensor.ravel())))/( normsqX))\n",
    "    # print('rec. error',preComputed[best_nfactors]['all_objs'][best_rep])\n",
    "    return shifted_tensor, fittensor\n",
    "\n",
    "def getNeuralMatrix(scld_permT, factors, lambdas, NDIRS, all_zeroed_stims=None,\n",
    "                    order='F', verbose=True):\n",
    "    \"\"\"Computes the final neural matrix, X, by fitting the permuted tensor scaled by\n",
    "    relative stimulus magnitudes using the factors obtained from NTF.\n",
    "    \n",
    "    Any previously zeroed out responses are now also permuted by the circular-shift\n",
    "    producing the best fit.\n",
    "    \n",
    "    Additionally, a rebalancing of the factor magnitudes is applied to attribute\n",
    "    a meaningful interpretation to the final coefficients.\n",
    "    \n",
    "    -------------------\n",
    "    Arguments:\n",
    "    \n",
    "    scld_permT: ndarray, permuted tensor scaled by relative stimulus FRs\n",
    "    \n",
    "    factors: list, [neural_factors, stimulus_factors, response_factors] (normalized)\n",
    "    \n",
    "    lambdas: ndarray, shape (R,), where R is the number of components being used\n",
    "    \n",
    "    NDIRS: int, number of stimulus directions (rows in original 2D response maps)\n",
    "    \n",
    "    all_zeroed_stims: dict, {cell: (tuple of zeroed stim idxs)}, default None\n",
    "    \n",
    "    order: str, order used to flatten the original 2D response maps, default 'F' \n",
    "    \n",
    "    -------------------\n",
    "    Returns:\n",
    "    X: ndarray, shape (Ncells, R), neural encoding matrix\n",
    "    \n",
    "    new_scld_permT: ndarray, tensor including previously zeroed out responses (if any)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    R = lambdas.size\n",
    "    \n",
    "    #rebalance factor loadings based on relative stimulus contributions + scale by lambdas\n",
    "    stim_factors = factors[1].copy()\n",
    "    stim_scls = stim_factors.max(0,keepdims=1)\n",
    "    stim_factors /= stim_scls\n",
    "\n",
    "    neural_factors = factors[0].copy()\n",
    "    neural_factors *= lambdas * stim_scls\n",
    "    \n",
    "    # rescaled stim x response coords\n",
    "    new_coords = np.stack([khatri_rao([stim_factors[:,r][:,None],factors[2][:,r][:,None]]).ravel() for r in range(R)],axis=1)\n",
    "\n",
    "    \n",
    "    Ncells = scld_permT.shape[0]\n",
    "    NSTIMS = scld_permT.shape[1]\n",
    "    \n",
    "    X = np.zeros((Ncells,R))\n",
    "    \n",
    "    new_scld_permT = scld_permT.copy()\n",
    "\n",
    "    for c in range(Ncells):\n",
    "        \n",
    "        if verbose and (c+1) % 50 == 0: print(c+1,end=' ')\n",
    "\n",
    "        if all_zeroed_stims is not None and c in all_zeroed_stims:\n",
    "            # Any previously zeroed out responses are now also permuted by the circular-shift\n",
    "            # producing the best fit.\n",
    "            \n",
    "            lowest_cost = np.inf\n",
    "            #for each shift of all zeroed-stims together\n",
    "            for shifti in range(NDIRS):\n",
    "                shifted_cell_data = scld_permT[c].copy()\n",
    "\n",
    "                for si in all_zeroed_stims[c]:\n",
    "                    #rotate orig_data\n",
    "                    si_2d = shifted_cell_data[si].reshape((NDIRS,-1),order=order)\n",
    "                    shifted_cell_data[si] = np.roll(si_2d,shifti,axis=0).ravel(order=order)\n",
    "\n",
    "                #compute fit cost\n",
    "                res = lsq_linear(new_coords,shifted_cell_data.ravel(),bounds=(0,np.inf))\n",
    "                coeffs, cost = res['x'], res['cost']\n",
    "\n",
    "                #if lower reconstruction cost, update best shift combo\n",
    "                if cost < lowest_cost:\n",
    "                    lowest_cost = cost\n",
    "                    best_shift = shifti\n",
    "                    best_coeffs = coeffs\n",
    "                    best_partial = True\n",
    "                    new_scld_permT[c] = shifted_cell_data\n",
    "\n",
    "            new_coeffs = best_coeffs\n",
    "\n",
    "                \n",
    "        else:#if no zeroed stims\n",
    "            # update coefficients to fit our stimulus-rescaled tensor\n",
    "            new_coeffs = lsq_linear(new_coords,scld_permT[c].ravel(),bounds=(0,np.inf))['x']\n",
    "\n",
    "        # sqrt so that, for each stimulus, the magnitude of a vector of coeffs for factors\n",
    "        # representing that stimulus can be equal to 1, even if that stimulus response \n",
    "        # is split across multiple factors. This ultimately leads to better distances\n",
    "        # between neurons\n",
    "        X[c] = np.sqrt(new_coeffs)\n",
    "\n",
    "    return X, new_scld_permT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load precomputed tensor files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDIRS = 8 #number of stimulus directions (rows in original 2-D response maps)\n",
    "\n",
    "# Load precomputed tensor and aux files (see `creating-the-tensor/creating-the-tensor.ipynb`)\n",
    "\n",
    "sigT = \n",
    "allT = \n",
    "all_zeroed_stims = \n",
    "cell_maxFRs = \n",
    "\n",
    "N = tensorX.shape[0]\n",
    "NSTIMS = tensorX.shape[1]\n",
    "RLEN = tensorX.shape[2]\n",
    "\n",
    "# Compute relative FRs between stimuli for each cell\n",
    "relFRs = [cell_maxFRs[c]/cell_maxFRs[c].max() for c in range(len(cell_maxFRs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load factorization results and compute neural encoding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed optimal factors and corresponding lambdas\n",
    "\n",
    "R = 17\n",
    "best_factors = np.load(f'cp-files/R{R}_factors.npy',allow_pickle=True)\n",
    "best_lambdas = np.load(f'cp-files/R{R}_lambdas.npy',allow_pickle=True)\n",
    "\n",
    "# remove any eventual zero-norm factor\n",
    "posnorms = ~np.isclose(best_lambdas,0)\n",
    "lambdas = best_lambdas[posnorms]\n",
    "# make sure they are all normalized\n",
    "factors = [f[:,posnorms]/np.linalg.norm(f[:,posnorms],axis=0,keepdims=1) for f in best_factors]\n",
    "\n",
    "# find the permuted version of sigT that gave rise to the factors -- this is\n",
    "# necessary for computing the actual rec error\n",
    "permT, fitT = getPermutedTensor(factors, lambdas, sigT)\n",
    "\n",
    "\n",
    "#now, add non-signif stims\n",
    "#note: these haven't been shifted by our factorization -- will address that later\n",
    "for c, zeroed_stims in all_zeroed_stims.items():\n",
    "    for si in zeroed_stims:\n",
    "        permT[c,si] = allT[c,si]\n",
    "        \n",
    "#finally, scale the (unit-normed) stimuli by their relative FRs\n",
    "scld_permT = permT * relFRs[...,None]\n",
    "\n",
    "# we will now proceed to adjust the neural loadings to reflect this, and to include the non-signif responses\n",
    "\n",
    "X, all_scld_permT = getNeuralMatrix(\n",
    "    scld_permT, factors, lambdas, NDIRS, all_zeroed_stims, order='F', verbose=False)\n",
    "\n",
    "# finally, eliminate possible redundancy among factors in the neural matrix using PCA\n",
    "# choose number of PCs based on a prespecified explained variance ratio\n",
    "MIN_EXPL_VAR_RATIO = 0.8\n",
    "\n",
    "pca = PCA(len(lambdas))\n",
    "newX = pca.fit_transform(X)\n",
    "nPCs = np.flatnonzero(np.cumsum(pca.explained_variance_ratio_) > MIN_EXPL_VAR_RATIO)[0] + 1\n",
    "print('nPCs',nPCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compute IAN similarity kernel from pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute matrix of squared distances\n",
    "D2 = squareform(pdist(newX[:,:nPCs], 'sqeuclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver = 'GUROBI' #using a commercial optimization package if highly recommended for faster kernel convergence\n",
    "# a free academic license can be obtained at https://www.gurobi.com/academia/academic-program-and-licenses/\n",
    "\n",
    "#use None if you don't have a preferred solver, or pick from the list of solvers from cvxpy: \n",
    "# https://www.cvxpy.org/tutorial/advanced/index.html#choosing-a-solver\n",
    "solver = None \n",
    "\n",
    "G, wG, optScales, disc_pts = IAN('exact-precomputed-sq', D2, solver=solver, plot_interval=0, verbose=0, plot_final_stats=0)\n",
    "\n",
    "# Optional: instead of picking a single factorization result, can compute separate graphs for each initialization and\n",
    "# average them together to combine all results into a single weighted graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Diffusion map embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ian.ian import * #https://github.com/dyballa/IAN/\n",
    "from ian.dset_utils import *\n",
    "\n",
    "# Compute diffusion map embedding using the IAN weighted graph as similarity matrix\n",
    "n_components = 3\n",
    "diffmap_y, diffmap_evals = diffusionMapFromK(wG, n_components)\n",
    "\n",
    "plot3dScatter(diffmap_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Local dimensionality estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate local dimension using NCD algorithm\n",
    "\n",
    "nbrhoodOrder = 2 #using neighbors-of-neighbors up to 2 hops away\n",
    "NofNDims, degDims = estimateLocalDims(G, D2, 1) \n",
    "dims = np.maximum(degDims,NofNDims)\n",
    "\n",
    "plot3dScatter(diffmap_y, dims)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:test0]",
   "language": "python",
   "name": "conda-env-test0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
